{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KX5Jz4mNnFNK",
        "outputId": "358be91b-e902-4eaa-d3ab-2a4666ac2d45"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]   Package conll2002 is already up-to-date!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Загальна кількість речень у навчальній вибірці: 7263\n",
            "Загальна кількість речень у тестовій вибірці: 1640\n",
            "Приклад речення: [('melbourne', 'NP'), ('(', 'Fpa'), ('australia', 'NP'), (')', 'Fpt'), (',', 'Fc'), ('25', 'Z'), ('may', 'NC'), ('(', 'Fpa'), ('efe', 'NC'), (')', 'Fpt'), ('.', 'Fp')]\n",
            "\n",
            "Розмір словника (слова, що зустрічаються >= 2 рази): 12574\n",
            "Підготовка даних завершена. Файли збережено.\n"
          ]
        }
      ],
      "source": [
        "import nltk\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Завантажуємо необхідні ресурси NLTK\n",
        "nltk.download('conll2002')\n",
        "\n",
        "# 1. Завантаження корпусу CONLL 2002 (іспанська частина)\n",
        "raw_train = nltk.corpus.conll2002.tagged_words('esp.train')\n",
        "raw_test = nltk.corpus.conll2002.tagged_words('esp.testa')\n",
        "\n",
        "# 2. ВИПРАВЛЕНА функція для перетворення даних у формат списку речень\n",
        "def structure_sentences(tagged_words):\n",
        "    \"\"\"\n",
        "    Структурує плаский список слів у список речень.\n",
        "    Ця версія стійка до неконсистентності даних (кортежі з 2 або 3 елементів).\n",
        "    \"\"\"\n",
        "    sentences = []\n",
        "    current_sentence = []\n",
        "    for item in tagged_words:\n",
        "        # Перевіряємо, чи має елемент 3 частини (word, pos, ner)\n",
        "        if len(item) == 3:\n",
        "            word, pos, ner = item\n",
        "        # чи лише 2 частини (word, pos)\n",
        "        elif len(item) == 2:\n",
        "            word, pos = item\n",
        "        else:\n",
        "            # Пропускаємо некоректні елементи\n",
        "            continue\n",
        "\n",
        "        current_sentence.append((word.lower(), pos))\n",
        "        # Розділяємо речення за крапкою\n",
        "        if word == '.':\n",
        "            sentences.append(current_sentence)\n",
        "            current_sentence = []\n",
        "\n",
        "    # Додаємо останнє речення, якщо воно залишилось\n",
        "    if current_sentence:\n",
        "        sentences.append(current_sentence)\n",
        "    return sentences\n",
        "\n",
        "train_sents = structure_sentences(raw_train)\n",
        "test_sents = structure_sentences(raw_test)\n",
        "\n",
        "print(f\"Загальна кількість речень у навчальній вибірці: {len(train_sents)}\")\n",
        "print(f\"Загальна кількість речень у тестовій вибірці: {len(test_sents)}\")\n",
        "print(\"Приклад речення:\", train_sents[0])\n",
        "\n",
        "# 3. Функція для запису даних у файли (залишається без змін)\n",
        "def write_tagged_sentences(sentences, filename):\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        for sentence in sentences:\n",
        "            if not sentence:\n",
        "                continue\n",
        "            for word, tag in sentence:\n",
        "                f.write(f\"{word}\\t{tag}\\n\")\n",
        "            f.write(\"\\n\")\n",
        "\n",
        "# Записуємо навчальну та тестову вибірки у файли\n",
        "write_tagged_sentences(train_sents, \"conll_training.pos\")\n",
        "write_tagged_sentences(test_sents, \"conll_test.pos\")\n",
        "\n",
        "# 4. Створення та збереження словника\n",
        "word_counter = Counter(word for sentence in train_sents for word, tag in sentence)\n",
        "vocab = {word for word, count in word_counter.items() if count >= 2}\n",
        "\n",
        "with open(\"conll_vocab.txt\", 'w', encoding='utf-8') as f:\n",
        "    for word in sorted(list(vocab)):\n",
        "        f.write(f\"{word}\\n\")\n",
        "\n",
        "print(f\"\\nРозмір словника (слова, що зустрічаються >= 2 рази): {len(vocab)}\")\n",
        "\n",
        "# 5. Створення файлу з тестовими словами\n",
        "with open(\"conll_test_words.txt\", 'w', encoding='utf-8') as f:\n",
        "    for sentence in test_sents:\n",
        "        if not sentence:\n",
        "            continue\n",
        "        for word, _ in sentence:\n",
        "            f.write(f\"{word}\\n\")\n",
        "        f.write(\"\\n\")\n",
        "\n",
        "print(\"Підготовка даних завершена. Файли збережено.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from collections import defaultdict\n",
        "\n",
        "def create_dictionaries(training_corpus_path, vocab):\n",
        "    \"\"\"\n",
        "    Створює словники для частот емісій, переходів та тегів.\n",
        "    \"\"\"\n",
        "    emission_counts = defaultdict(int)\n",
        "    transition_counts = defaultdict(int)\n",
        "    tag_counts = defaultdict(int)\n",
        "    prev_tag = '--s--'\n",
        "\n",
        "    with open(training_corpus_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                transition_counts[(prev_tag, '--s--')] += 1\n",
        "                prev_tag = '--s--'\n",
        "                continue\n",
        "\n",
        "            word, tag = line.split('\\t')\n",
        "\n",
        "            # У цій версії ми не обробляємо невідомі слова тут,\n",
        "            # оскільки це буде зроблено пізніше в алгоритмі Вітербі.\n",
        "            # Однак, для підрахунку емісій, ми можемо замінити їх на '--unk--'\n",
        "            word_to_count = word if word in vocab else '--unk--'\n",
        "\n",
        "            transition_counts[(prev_tag, tag)] += 1\n",
        "            emission_counts[(tag, word_to_count)] += 1\n",
        "            tag_counts[tag] += 1\n",
        "            prev_tag = tag\n",
        "\n",
        "    return emission_counts, transition_counts, tag_counts\n",
        "\n",
        "# Створюємо словники на основі НОВИХ навчальних даних\n",
        "emission_counts, transition_counts, tag_counts = create_dictionaries(\"conll_training.pos\", vocab)\n",
        "\n",
        "# Додаємо тег початку речення до лічильника тегів\n",
        "tag_counts['--s--'] = len(train_sents)\n",
        "\n",
        "print(\"Приклад частоти емісії:\", list(emission_counts.items())[0])\n",
        "print(\"Приклад частоти переходу:\", list(transition_counts.items())[0])\n",
        "print(\"Приклад частоти тега:\", list(tag_counts.items())[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4sVKqMcEoL0q",
        "outputId": "13ba64e1-366e-410d-d247-4c1ccc056436"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Приклад частоти емісії: (('NP', 'melbourne'), 2)\n",
            "Приклад частоти переходу: (('--s--', 'NP'), 319)\n",
            "Приклад частоти тега: ('NP', 1150)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "# Крок 3: Матриця переходів A\n",
        "def create_transition_matrix(alpha, tag_counts, transition_counts):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    tag_to_idx = {tag: i for i, tag in enumerate(all_tags)}\n",
        "    idx_to_tag = {i: tag for i, tag in enumerate(all_tags)}\n",
        "    num_tags = len(all_tags)\n",
        "    A = np.zeros((num_tags, num_tags))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        prev_tag = idx_to_tag[i]\n",
        "        count_prev_tag = tag_counts[prev_tag]\n",
        "        for j in range(num_tags):\n",
        "            tag = idx_to_tag[j]\n",
        "            count_transition = transition_counts.get((prev_tag, tag), 0)\n",
        "            A[i, j] = (count_transition + alpha) / (count_prev_tag + alpha * num_tags)\n",
        "    return A, tag_to_idx, idx_to_tag\n",
        "\n",
        "alpha = 0.001\n",
        "A, tag_to_idx, idx_to_tag = create_transition_matrix(alpha, tag_counts, transition_counts)\n",
        "print(\"Розмір матриці переходів A:\", A.shape)\n",
        "\n",
        "\n",
        "# Крок 4: Матриця емісій B\n",
        "def create_emission_matrix(alpha, tag_counts, emission_counts, vocab):\n",
        "    all_tags = sorted(tag_counts.keys())\n",
        "    num_tags = len(all_tags)\n",
        "    all_words = sorted(list(vocab)) + ['--unk--']\n",
        "    word_to_idx = {word: i for i, word in enumerate(all_words)}\n",
        "    idx_to_word = {i: word for i, word in enumerate(all_words)}\n",
        "    num_words = len(all_words)\n",
        "    B = np.zeros((num_tags, num_words))\n",
        "\n",
        "    for i in range(num_tags):\n",
        "        tag = all_tags[i]\n",
        "        count_tag = tag_counts[tag]\n",
        "        for j in range(num_words):\n",
        "            word = idx_to_word[j]\n",
        "            count_emission = emission_counts.get((tag, word), 0)\n",
        "            B[i, j] = (count_emission + alpha) / (count_tag + alpha * num_words)\n",
        "    return B, word_to_idx, idx_to_word\n",
        "\n",
        "B, word_to_idx, idx_to_word = create_emission_matrix(alpha, tag_counts, emission_counts, vocab)\n",
        "print(\"Розмір матриці емісій B:\", B.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "syIW0ULjoObe",
        "outputId": "33b01920-f575-4394-b360-68c89c152fd4"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Розмір матриці переходів A: (60, 60)\n",
            "Розмір матриці емісій B: (60, 12575)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Крок 5: Алгоритм Вітербі\n",
        "import math\n",
        "\n",
        "def viterbi(sentence, tag_to_idx, word_to_idx, A, B):\n",
        "    num_tags = len(tag_to_idx)\n",
        "    T = len(sentence)\n",
        "    viterbi_matrix = np.full((num_tags, T), -np.inf)\n",
        "    backpointer = np.zeros((num_tags, T), dtype=int)\n",
        "    start_tag_idx = tag_to_idx['--s--']\n",
        "    first_word_idx = word_to_idx.get(sentence[0], word_to_idx['--unk--'])\n",
        "\n",
        "    for tag_idx in range(num_tags):\n",
        "        if A[start_tag_idx, tag_idx] > 0 and B[tag_idx, first_word_idx] > 0:\n",
        "            viterbi_matrix[tag_idx, 0] = math.log(A[start_tag_idx, tag_idx]) + math.log(B[tag_idx, first_word_idx])\n",
        "\n",
        "    for t in range(1, T):\n",
        "        word_idx = word_to_idx.get(sentence[t], word_to_idx['--unk--'])\n",
        "        for j in range(num_tags):\n",
        "            max_prob = -np.inf\n",
        "            best_prev_tag_idx = -1\n",
        "            for i in range(num_tags):\n",
        "                if A[i, j] > 0 and B[j, word_idx] > 0:\n",
        "                    prob = viterbi_matrix[i, t-1] + math.log(A[i, j]) + math.log(B[j, word_idx])\n",
        "                    if prob > max_prob:\n",
        "                        max_prob = prob\n",
        "                        best_prev_tag_idx = i\n",
        "            viterbi_matrix[j, t] = max_prob\n",
        "            backpointer[j, t] = best_prev_tag_idx\n",
        "\n",
        "    best_path = []\n",
        "    last_tag_idx = np.argmax(viterbi_matrix[:, T-1])\n",
        "    best_path.append(idx_to_tag[last_tag_idx])\n",
        "    for t in range(T-1, 0, -1):\n",
        "        last_tag_idx = backpointer[last_tag_idx, t]\n",
        "        best_path.insert(0, idx_to_tag[last_tag_idx])\n",
        "    return best_path\n",
        "\n",
        "# Крок 6: Оцінка точності\n",
        "from tqdm import tqdm\n",
        "\n",
        "def compute_accuracy(test_sents, tag_to_idx, word_to_idx, A, B):\n",
        "    num_correct = 0\n",
        "    total = 0\n",
        "    for sentence_tags in tqdm(test_sents, desc=\"Оцінка точності\"):\n",
        "        if not sentence_tags: continue\n",
        "        sentence = [word for word, tag in sentence_tags]\n",
        "        gold_tags = [tag for word, tag in sentence_tags]\n",
        "        predicted_tags = viterbi(sentence, tag_to_idx, word_to_idx, A, B)\n",
        "        for pred_tag, gold_tag in zip(predicted_tags, gold_tags):\n",
        "            if pred_tag == gold_tag:\n",
        "                num_correct += 1\n",
        "            total += 1\n",
        "    return num_correct / total\n",
        "\n",
        "accuracy = compute_accuracy(test_sents, tag_to_idx, word_to_idx, A, B)\n",
        "print(f\"\\nТочність реалізованої моделі на тестовій вибірці (CONLL 2002): {accuracy:.4f}\")\n",
        "\n",
        "\n",
        "# Крок 7: Порівняння з NLTK\n",
        "from nltk.tag import DefaultTagger, UnigramTagger, BigramTagger\n",
        "\n",
        "most_common_tag = Counter(tag for sent in train_sents for word, tag in sent).most_common(1)[0][0]\n",
        "default_tagger = DefaultTagger(most_common_tag)\n",
        "unigram_tagger = UnigramTagger(train_sents, backoff=default_tagger)\n",
        "bigram_tagger = BigramTagger(train_sents, backoff=unigram_tagger)\n",
        "nltk_accuracy = bigram_tagger.accuracy(test_sents)\n",
        "\n",
        "print(f\"\\nТочність NLTK BigramTagger: {nltk_accuracy:.4f}\")\n",
        "print(f\"Точність нашої реалізації: {accuracy:.4f}\")\n",
        "print(f\"Різниця: {abs(accuracy - nltk_accuracy):.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3lhuA0fZoQlr",
        "outputId": "3546405d-397a-4be1-d0da-bb2c9c93f61d"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Оцінка точності: 100%|██████████| 1640/1640 [04:40<00:00,  5.84it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Точність реалізованої моделі на тестовій вибірці (CONLL 2002): 0.9352\n",
            "\n",
            "Точність NLTK BigramTagger: 0.9211\n",
            "Точність нашої реалізації: 0.9352\n",
            "Різниця: 0.0141\n"
          ]
        }
      ]
    }
  ]
}

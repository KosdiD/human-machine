{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Vop8AMvl0vVM",
        "outputId": "792ad037-d7f7-47dc-fd69-5e907b2790db"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.6 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m54.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hСередовище готове.\n"
          ]
        }
      ],
      "source": [
        "# Встановлення необхідних бібліотек\n",
        "!pip install -q nltk ipywidgets\n",
        "\n",
        "# Імпорт бібліотек\n",
        "import re\n",
        "import random\n",
        "from collections import Counter\n",
        "import numpy as np\n",
        "import nltk\n",
        "import ipywidgets as widgets\n",
        "from IPython.display import display, clear_output\n",
        "\n",
        "# Завантаження ресурсів NLTK\n",
        "nltk.download('punkt', quiet=True)\n",
        "\n",
        "print(\"Середовище готове.\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Крок 1: Завантаження та розархівування (цей крок потрібно дочекатися) ---\n",
        "\n",
        "# 1. Завантажуємо ПОВНИЙ файл корпусу новин. Це може зайняти час.\n",
        "!wget https://lang.org.ua/static/downloads/corpora/news.tokenized.shuffled.txt.bz2\n",
        "\n",
        "# 2. Розархівовуємо завантажений файл\n",
        "!bzip2 -dk news.tokenized.shuffled.txt.bz2\n",
        "\n",
        "print(\"\\nКорпус новин завантажено та розархівовано. Починаємо часткову обробку...\")\n",
        "\n",
        "\n",
        "# --- Крок 2: Часткова обробка даних ---\n",
        "\n",
        "# Імпортуємо необхідні бібліотеки\n",
        "import re\n",
        "import random\n",
        "from collections import Counter\n",
        "\n",
        "# Допоміжні функції (вони залишаються без змін)\n",
        "def count_words(tokenized_sentences):\n",
        "    word_counts = Counter()\n",
        "    for sentence in tokenized_sentences:\n",
        "        word_counts.update(sentence)\n",
        "    return word_counts\n",
        "\n",
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    closed_vocab = [word for word, cnt in word_counts.items() if cnt >= count_threshold]\n",
        "    return closed_vocab\n",
        "\n",
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    vocabulary = set(vocabulary)\n",
        "    replaced_tokenized_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        replaced_sentence = [token if token in vocabulary else unknown_token for token in sentence]\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences\n",
        "\n",
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
        "    return train_data_replaced, test_data_replaced, vocabulary\n",
        "\n",
        "# Функція, яка читає лише частину файлу\n",
        "def load_and_process_partially(filepath, num_lines_to_process):\n",
        "    tokenized_sentences = []\n",
        "    print(f\"Починаємо читати та обробляти перші {num_lines_to_process} рядків з файлу...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= num_lines_to_process:\n",
        "                    break\n",
        "                sentence = line.lower().strip()\n",
        "                tokens = sentence.split()\n",
        "                if tokens:\n",
        "                    tokenized_sentences.append(tokens)\n",
        "                if (i + 1) % 100000 == 0:\n",
        "                    print(f\"  Оброблено {i + 1} рядків...\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Помилка: файл '{filepath}' не знайдено.\")\n",
        "        return []\n",
        "    print(f\"Завершено. Оброблено {len(tokenized_sentences)} речень.\")\n",
        "    return tokenized_sentences\n",
        "\n",
        "LINES_TO_PROCESS = 200000\n",
        "corpus_path = 'news.tokenized.shuffled.txt'\n",
        "\n",
        "tokenized_data = load_and_process_partially(corpus_path, LINES_TO_PROCESS)\n",
        "\n",
        "if tokenized_data:\n",
        "    random.seed(42)\n",
        "    random.shuffle(tokenized_data)\n",
        "    train_size = int(len(tokenized_data) * 0.8)\n",
        "    train_data = tokenized_data[:train_size]\n",
        "    test_data = tokenized_data[train_size:]\n",
        "\n",
        "    print(f\"\\nРозмір тренувальної вибірки: {len(train_data)} речень\")\n",
        "    print(f\"Розмір тестової вибірки: {len(test_data)} речень\")\n",
        "\n",
        "    print(\"\\nПопередня обробка та створення словника...\")\n",
        "    count_threshold = 2\n",
        "    train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, count_threshold)\n",
        "\n",
        "    print(f\"Розмір словника: {len(vocabulary)} слів\")\n",
        "    print(\"\\nПідготовка даних успішно завершена! Можна переходити до наступних кроків.\")\n",
        "else:\n",
        "    print(\"Не вдалося обробити дані.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rQko9t-20z1b",
        "outputId": "56baeec1-9053-40e2-a3f3-6cba6fe223fe"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-06-28 14:07:34--  https://lang.org.ua/static/downloads/corpora/news.tokenized.shuffled.txt.bz2\n",
            "Resolving lang.org.ua (lang.org.ua)... 65.21.91.242\n",
            "Connecting to lang.org.ua (lang.org.ua)|65.21.91.242|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1107980123 (1.0G) [application/octet-stream]\n",
            "Saving to: ‘news.tokenized.shuffled.txt.bz2.1’\n",
            "\n",
            "news.tokenized.shuf 100%[===================>]   1.03G  30.6MB/s    in 35s     \n",
            "\n",
            "2025-06-28 14:08:09 (30.1 MB/s) - ‘news.tokenized.shuffled.txt.bz2.1’ saved [1107980123/1107980123]\n",
            "\n",
            "bzip2: Output file news.tokenized.shuffled.txt already exists.\n",
            "\n",
            "Корпус новин завантажено та розархівовано. Починаємо часткову обробку...\n",
            "Починаємо читати та обробляти перші 200000 рядків з файлу...\n",
            "  Оброблено 100000 рядків...\n",
            "  Оброблено 200000 рядків...\n",
            "Завершено. Оброблено 200000 речень.\n",
            "\n",
            "Розмір тренувальної вибірки: 160000 речень\n",
            "Розмір тестової вибірки: 40000 речень\n",
            "\n",
            "Попередня обробка та створення словника...\n",
            "Розмір словника: 87381 слів\n",
            "\n",
            "Підготовка даних успішно завершена! Можна переходити до наступних кроків.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- Допоміжні функції для обробки даних ---\n",
        "\n",
        "def count_words(tokenized_sentences):\n",
        "    \"\"\"Підраховує частоту кожного слова в корпусі.\"\"\"\n",
        "    word_counts = Counter()\n",
        "    for sentence in tokenized_sentences:\n",
        "        word_counts.update(sentence)\n",
        "    return word_counts\n",
        "\n",
        "def get_words_with_nplus_frequency(tokenized_sentences, count_threshold):\n",
        "    \"\"\"Отримує слова, які з'являються не менше count_threshold разів.\"\"\"\n",
        "    word_counts = count_words(tokenized_sentences)\n",
        "    closed_vocab = [word for word, cnt in word_counts.items() if cnt >= count_threshold]\n",
        "    return closed_vocab\n",
        "\n",
        "def replace_oov_words_by_unk(tokenized_sentences, vocabulary, unknown_token=\"<unk>\"):\n",
        "    \"\"\"Замінює слова, що не входять до словника, на токен unknown_token.\"\"\"\n",
        "    vocabulary = set(vocabulary)\n",
        "    replaced_tokenized_sentences = []\n",
        "    for sentence in tokenized_sentences:\n",
        "        replaced_sentence = [token if token in vocabulary else unknown_token for token in sentence]\n",
        "        replaced_tokenized_sentences.append(replaced_sentence)\n",
        "    return replaced_tokenized_sentences\n",
        "\n",
        "def preprocess_data(train_data, test_data, count_threshold):\n",
        "    \"\"\"Виконує повну попередню обробку даних.\"\"\"\n",
        "    vocabulary = get_words_with_nplus_frequency(train_data, count_threshold)\n",
        "    train_data_replaced = replace_oov_words_by_unk(train_data, vocabulary)\n",
        "    test_data_replaced = replace_oov_words_by_unk(test_data, vocabulary)\n",
        "    return train_data_replaced, test_data_replaced, vocabulary\n",
        "\n",
        "# --- Оптимізована функція для читання великого файлу ---\n",
        "\n",
        "def load_and_process_partially(filepath, num_lines_to_process):\n",
        "    tokenized_sentences = []\n",
        "    print(f\"Починаємо читати та обробляти перші {num_lines_to_process} рядків з файлу...\")\n",
        "    try:\n",
        "        with open(filepath, 'r', encoding='utf-8') as f:\n",
        "            for i, line in enumerate(f):\n",
        "                if i >= num_lines_to_process:\n",
        "                    break\n",
        "                sentence = line.lower().strip()\n",
        "                tokens = sentence.split()\n",
        "                if tokens:\n",
        "                    tokenized_sentences.append(tokens)\n",
        "                if (i + 1) % 200000 == 0:\n",
        "                    print(f\"  Оброблено {i + 1} рядків...\")\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Помилка: файл '{filepath}' не знайдено.\")\n",
        "        return []\n",
        "    print(f\"Завершено. Оброблено {len(tokenized_sentences)} речень.\")\n",
        "    return tokenized_sentences\n",
        "\n",
        "# --- Основна частина запуску обробки ---\n",
        "\n",
        "LINES_TO_PROCESS = 500000  # Можна налаштувати кількість рядків для обробки\n",
        "corpus_path = 'news.tokenized.shuffled.txt'\n",
        "\n",
        "tokenized_data = load_and_process_partially(corpus_path, LINES_TO_PROCESS)\n",
        "\n",
        "if tokenized_data:\n",
        "    random.seed(42) # для відтворюваності\n",
        "    random.shuffle(tokenized_data)\n",
        "    train_size = int(len(tokenized_data) * 0.8)\n",
        "    train_data = tokenized_data[:train_size]\n",
        "    test_data = tokenized_data[train_size:]\n",
        "\n",
        "    print(f\"\\nРозмір тренувальної вибірки: {len(train_data)} речень\")\n",
        "    print(f\"Розмір тестової вибірки: {len(test_data)} речень\")\n",
        "\n",
        "    print(\"\\nПопередня обробка та створення словника...\")\n",
        "    count_threshold = 2\n",
        "    train_data_processed, test_data_processed, vocabulary = preprocess_data(train_data, test_data, count_threshold)\n",
        "\n",
        "    print(f\"Розмір словника: {len(vocabulary)} слів\")\n",
        "    print(\"\\nКрок 2 успішно завершено!\")\n",
        "else:\n",
        "    print(\"Не вдалося обробити дані.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCnTTHbV00or",
        "outputId": "11202be4-adcb-4b64-8818-4284a8ae1ec9"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Починаємо читати та обробляти перші 500000 рядків з файлу...\n",
            "  Оброблено 200000 рядків...\n",
            "  Оброблено 400000 рядків...\n",
            "Завершено. Оброблено 500000 речень.\n",
            "\n",
            "Розмір тренувальної вибірки: 400000 речень\n",
            "Розмір тестової вибірки: 100000 речень\n",
            "\n",
            "Попередня обробка та створення словника...\n",
            "Розмір словника: 151073 слів\n",
            "\n",
            "Крок 2 успішно завершено!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def count_n_grams(data, n, start_token='<s>', end_token='</s>'):\n",
        "    \"\"\"Підраховує кількість n-грам у даних.\"\"\"\n",
        "    n_grams = Counter()\n",
        "    for sentence in data:\n",
        "        sentence = [start_token] * (n - 1) + sentence + [end_token]\n",
        "        sentence = tuple(sentence)\n",
        "        for i in range(len(sentence) - n + 1):\n",
        "            n_gram = sentence[i:i+n]\n",
        "            n_grams[n_gram] += 1\n",
        "    return n_grams\n",
        "\n",
        "# Обчислюємо N-грами різних розмірів\n",
        "print(\"Обчислення N-грам...\")\n",
        "n_gram_counts_list = []\n",
        "for n in range(1, 6): # Від уніграм (n=1) до 5-грам (n=5)\n",
        "    print(f\"Побудова {n}-грам...\")\n",
        "    n_grams = count_n_grams(train_data_processed, n)\n",
        "    n_gram_counts_list.append(n_grams)\n",
        "\n",
        "print(\"N-грамні моделі успішно побудовано.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rCsKp3RG04P0",
        "outputId": "7e44c47b-7423-48c5-dc1e-bb6ec4444b63"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Обчислення N-грам...\n",
            "Побудова 1-грам...\n",
            "Побудова 2-грам...\n",
            "Побудова 3-грам...\n",
            "Побудова 4-грам...\n",
            "Побудова 5-грам...\n",
            "N-грамні моделі успішно побудовано.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def estimate_probability(word, previous_n_gram, n_gram_counts, n_plus1_gram_counts, vocabulary_size, k=1.0):\n",
        "    \"\"\"Оцінює ймовірність слова word, враховуючи попередню n-граму.\"\"\"\n",
        "    previous_n_gram = tuple(previous_n_gram)\n",
        "    previous_n_gram_count = n_gram_counts.get(previous_n_gram, 0)\n",
        "    denominator = previous_n_gram_count + k * vocabulary_size\n",
        "    n_plus1_gram = previous_n_gram + (word,)\n",
        "    n_plus1_gram_count = n_plus1_gram_counts.get(n_plus1_gram, 0)\n",
        "    numerator = n_plus1_gram_count + k\n",
        "    probability = numerator / denominator\n",
        "    return probability\n",
        "\n",
        "def calculate_perplexity(sentence, n_gram_counts_list, vocabulary_size, k=1.0):\n",
        "    \"\"\"Обчислює перплексію для речення.\"\"\"\n",
        "    n = len(n_gram_counts_list)\n",
        "    n_minus_1_gram_counts = n_gram_counts_list[n-2]\n",
        "    n_gram_counts = n_gram_counts_list[n-1]\n",
        "\n",
        "    sentence = ['<s>'] * (n - 1) + sentence + ['</s>']\n",
        "    sentence = tuple(sentence)\n",
        "    N = len(sentence)\n",
        "    product_pi = 1.0\n",
        "\n",
        "    for t in range(n - 1, N):\n",
        "        previous_n_gram = sentence[t-(n-1):t]\n",
        "        word = sentence[t]\n",
        "        probability = estimate_probability(word, previous_n_gram, n_minus_1_gram_counts, n_gram_counts, vocabulary_size, k)\n",
        "\n",
        "        if probability == 0:\n",
        "            return float('inf')\n",
        "\n",
        "        product_pi *= (1 / probability)\n",
        "\n",
        "    perplexity = product_pi**(1/float(N))\n",
        "    return perplexity\n",
        "\n",
        "# Оцінимо перплексію нашої 5-грамної моделі на одному реченні\n",
        "print(\"\\nОцінка перплексії...\")\n",
        "if test_data_processed:\n",
        "    example_sentence = test_data_processed[0]\n",
        "    # Використовуємо 4-грами та 5-грами для розрахунку ймовірності 5-грамної моделі\n",
        "    perplexity = calculate_perplexity(example_sentence, [n_gram_counts_list[3], n_gram_counts_list[4]], len(vocabulary))\n",
        "    print(f\"Речення: {' '.join(example_sentence)}\")\n",
        "    print(f\"Перплексія для 5-грамної моделі: {perplexity:.4f}\")\n",
        "else:\n",
        "    print(\"Тестові дані порожні, неможливо розрахувати перплексію.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DCTAInCY4M3P",
        "outputId": "3ee248e6-9a82-40e3-9b37-3efb49b73f04"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Оцінка перплексії...\n",
            "Речення: всього до євро-2012 треба 14\n",
            "Перплексія для 5-грамної моделі: 27498.1209\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def get_suggestions(previous_tokens, n_gram_counts_list, vocabulary, k=1.0, start_with=None):\n",
        "    \"\"\"Отримує список найімовірніших наступних слів від різних N-грамних моделей.\"\"\"\n",
        "    model_suggestions = {}\n",
        "    n_max = len(n_gram_counts_list)\n",
        "\n",
        "    # Функція для оцінки ймовірностей для однієї N-грамної моделі\n",
        "    def estimate_probabilities_for_model(prev_gram, n_minus_1_counts, n_counts, vocab, vocab_size, k_val):\n",
        "        probabilities = {}\n",
        "        for word in vocab:\n",
        "            prob = estimate_probability(word, prev_gram, n_minus_1_counts, n_counts, vocab_size, k_val)\n",
        "            probabilities[word] = prob\n",
        "        return probabilities\n",
        "\n",
        "    vocab_with_special_tokens = vocabulary + [\"<unk>\", \"</s>\"]\n",
        "    vocabulary_size = len(vocab_with_special_tokens)\n",
        "\n",
        "    # Ітеруємо від найдовших N-грам до найкоротших (backoff)\n",
        "    for n in range(n_max, 1, -1):\n",
        "        n_minus_1_counts = n_gram_counts_list[n-2]\n",
        "        n_counts = n_gram_counts_list[n-1]\n",
        "\n",
        "        # Визначаємо контекст\n",
        "        context_len = n - 1\n",
        "        previous_n_gram = previous_tokens[-context_len:] if len(previous_tokens) >= context_len else [\"<s>\"] * (context_len - len(previous_tokens)) + previous_tokens\n",
        "\n",
        "        # Обчислюємо ймовірності\n",
        "        probabilities = estimate_probabilities_for_model(tuple(previous_n_gram), n_minus_1_counts, n_counts, vocab_with_special_tokens, vocabulary_size, k)\n",
        "\n",
        "        # Додаємо пропозиції, якщо вони ще не були додані від довшої моделі\n",
        "        for word, prob in probabilities.items():\n",
        "            if start_with and not word.startswith(start_with):\n",
        "                continue\n",
        "            if word not in model_suggestions:\n",
        "                model_suggestions[word] = prob\n",
        "\n",
        "    # Сортуємо слова за їхньою ймовірністю\n",
        "    sorted_suggestions = sorted(model_suggestions.items(), key=lambda x: x[1], reverse=True)\n",
        "    return sorted_suggestions\n",
        "\n",
        "# Приклад використання\n",
        "previous_text = \"міністерство закордонних справ\"\n",
        "tokens = previous_text.split()\n",
        "suggestions = get_suggestions(tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "print(f\"\\nПропозиції для тексту '{previous_text}':\")\n",
        "# Фільтруємо спец-токени перед виводом\n",
        "filtered_suggestions = [s for s in suggestions if s[0] not in ['<unk>', '<s>', '</s>']]\n",
        "for word, prob in filtered_suggestions[:5]:\n",
        "    print(f\"- {word} (ймовірність: {prob:.6f})\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cto8tC184RaR",
        "outputId": "3674448a-e550-4729-df78-c9c03068077a"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Пропозиції для тексту 'міністерство закордонних справ':\n",
            "- україни (ймовірність: 0.000172)\n",
            "- росії (ймовірність: 0.000033)\n",
            "- польщі (ймовірність: 0.000026)\n",
            "- мзс (ймовірність: 0.000026)\n",
            "- висловлює (ймовірність: 0.000013)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def run_interactive_console(n_gram_counts_list, vocabulary):\n",
        "    \"\"\"\n",
        "    Запускає простий текстовий інтерфейс для тестування автозавершення.\n",
        "    \"\"\"\n",
        "    print(\"\\n--- Інтерактивний режим автозавершення ---\")\n",
        "    print(\"Введіть текст. Для виходу напишіть 'вихід' або 'exit'.\")\n",
        "\n",
        "    while True:\n",
        "        try:\n",
        "            text = input(\"\\nВаш текст: \")\n",
        "        except (KeyboardInterrupt, EOFError):\n",
        "            print(\"\\nЗавершення роботи.\")\n",
        "            break\n",
        "\n",
        "        if text.lower().strip() in ['вихід', 'exit']:\n",
        "            print(\"Завершення роботи.\")\n",
        "            break\n",
        "\n",
        "        if not text:\n",
        "            continue\n",
        "\n",
        "        tokens = text.lower().strip().split()\n",
        "\n",
        "        # Отримуємо пропозиції\n",
        "        suggestions = get_suggestions(tokens, n_gram_counts_list, vocabulary, k=1.0)\n",
        "\n",
        "        # Фільтруємо спец-токени ПЕРЕД виводом\n",
        "        final_suggestions = [s for s in suggestions if s[0] not in ['<unk>', '<s>', '</s>']]\n",
        "\n",
        "        if not final_suggestions:\n",
        "            print(\"Немає пропозицій для даного контексту.\")\n",
        "        else:\n",
        "            print(\"Найкращі пропозиції:\")\n",
        "            for word, prob in final_suggestions[:5]:\n",
        "                print(f\"- {word} (ймовірність: {prob:.6f})\")\n",
        "\n",
        "# Запуск інтерактивного інтерфейсу\n",
        "# Переконуємось, що дані були успішно оброблені\n",
        "if 'n_gram_counts_list' in locals() and 'vocabulary' in locals():\n",
        "    run_interactive_console(n_gram_counts_list, vocabulary)\n",
        "else:\n",
        "    print(\"Помилка: моделі не були навчені. Перевірте виконання попередніх кроків.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QH8phqr-4U3V",
        "outputId": "52d5ece6-761d-4776-9bab-f547128afda4"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Інтерактивний режим автозавершення ---\n",
            "Введіть текст. Для виходу напишіть 'вихід' або 'exit'.\n",
            "\n",
            "Ваш текст: На даний момент\n",
            "Найкращі пропозиції:\n",
            "- у (ймовірність: 0.000060)\n",
            "- в (ймовірність: 0.000046)\n",
            "- на (ймовірність: 0.000026)\n",
            "- проходить (ймовірність: 0.000026)\n",
            "- відомо (ймовірність: 0.000026)\n",
            "\n",
            "Ваш текст: В такому випадку рекомендовано\n",
            "Найкращі пропозиції:\n",
            "- проте (ймовірність: 0.000007)\n",
            "- з (ймовірність: 0.000007)\n",
            "- цими (ймовірність: 0.000007)\n",
            "- явищами (ймовірність: 0.000007)\n",
            "- за (ймовірність: 0.000007)\n",
            "\n",
            "Ваш текст: вихід\n",
            "Завершення роботи.\n"
          ]
        }
      ]
    }
  ]
}
{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "import random\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "nNyYM2HC34NR"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('senseval')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47vsjuwmMnwI",
        "outputId": "2e1bd32b-74f3-4be3-9dac-8dbac8a3b958"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]   Package senseval is already up-to-date!\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from nltk.corpus import stopwords, senseval\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "\n",
        "# --- Функція попередньої обробки тексту (без змін) ---\n",
        "def process_text(text_tokens):\n",
        "    stemmer = PorterStemmer()\n",
        "    stop_words = set(stopwords.words('english'))\n",
        "    cleaned_tokens = []\n",
        "    for token in text_tokens:\n",
        "        token = token.lower()\n",
        "        if token.isalpha() and token not in stop_words:\n",
        "            stemmed_token = stemmer.stem(token)\n",
        "            cleaned_tokens.append(stemmed_token)\n",
        "    return cleaned_tokens\n",
        "\n",
        "# --- Реалізація Наївного Баєсового класифікатора (без змін) ---\n",
        "def build_freqs(texts, labels):\n",
        "    freqs = {}\n",
        "    for y, text in zip(labels, texts):\n",
        "        for word in process_text(text):\n",
        "            pair = (word, y)\n",
        "            freqs[pair] = freqs.get(pair, 0) + 1\n",
        "    return freqs\n",
        "\n",
        "def train_naive_bayes(freqs, train_y):\n",
        "    loglikelihood = {}\n",
        "    logprior = 0\n",
        "    vocab = set([pair[0] for pair in freqs.keys()])\n",
        "    V = len(vocab)\n",
        "    N_pos = N_neg = 0\n",
        "    for pair, count in freqs.items():\n",
        "        if pair[1] == 1:\n",
        "            N_pos += count\n",
        "        else:\n",
        "            N_neg += count\n",
        "    D = len(train_y)\n",
        "    D_pos = np.sum(train_y)\n",
        "    D_neg = D - D_pos\n",
        "    logprior = np.log(D_pos) - np.log(D_neg)\n",
        "    for word in vocab:\n",
        "        freq_pos = freqs.get((word, 1.0), 0)\n",
        "        freq_neg = freqs.get((word, 0.0), 0)\n",
        "        p_w_pos = (freq_pos + 1) / (N_pos + V)\n",
        "        p_w_neg = (freq_neg + 1) / (N_neg + V)\n",
        "        loglikelihood[word] = np.log(p_w_pos / p_w_neg)\n",
        "    return logprior, loglikelihood\n",
        "\n",
        "def naive_bayes_predict(text_tokens, logprior, loglikelihood):\n",
        "    word_l = process_text(text_tokens)\n",
        "    p = logprior\n",
        "    for word in word_l:\n",
        "        if word in loglikelihood:\n",
        "            p += loglikelihood[word]\n",
        "    return p\n",
        "\n",
        "def test_naive_bayes(test_x, test_y, logprior, loglikelihood):\n",
        "    y_hats = []\n",
        "    for text in test_x:\n",
        "        if naive_bayes_predict(text, logprior, loglikelihood) > 0:\n",
        "            y_hat_i = 1\n",
        "        else:\n",
        "            y_hat_i = 0\n",
        "        y_hats.append(y_hat_i)\n",
        "    error = np.mean(np.abs(np.array(y_hats) - np.array(test_y)))\n",
        "    accuracy = 1 - error\n",
        "    return accuracy\n"
      ],
      "metadata": {
        "id": "GnE_sBiC3FVV"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Завантаження даних з корпусу senseval (ВИПРАВЛЕНА ВЕРСІЯ)\n",
        "print(\"\\nКрок 1: Завантаження та підготовка даних з 'senseval'...\")\n",
        "documents = []\n",
        "\n",
        "# Функція для витягнення слів з контексту (ВИПРАВЛЕНА ВЕРСІЯ)\n",
        "def extract_words_from_context(context):\n",
        "    # ВИПРАВЛЕННЯ ТУТ: Беремо перший елемент з кожного кортежу,\n",
        "    # незалежно від його довжини.\n",
        "    return [item[0] for item in context]\n",
        "\n",
        "for inst in senseval.instances('hard.pos'):\n",
        "    context_words = extract_words_from_context(inst.context)\n",
        "    documents.append((context_words, 0))\n",
        "\n",
        "for inst in senseval.instances('interest.pos'):\n",
        "    context_words = extract_words_from_context(inst.context)\n",
        "    documents.append((context_words, 1))\n",
        "\n",
        "random.shuffle(documents)\n",
        "\n",
        "print(f\"Завантажено {len(documents)} речень.\")\n",
        "print(\"Приклад речення (для слова 'hard' або 'interest'):\")\n",
        "print(f\"Текст: {' '.join(documents[0][0])}\")\n",
        "print(f\"Мітка класу: {documents[0][1]} (0=hard, 1=interest)\")\n",
        "\n",
        "\n",
        "# 2. Розділення на тренувальний та тестовий набори\n",
        "print(\"\\nКрок 2: Розділення даних...\")\n",
        "split_ratio = int(len(documents) * 0.8)\n",
        "train_documents = documents[:split_ratio]\n",
        "test_documents = documents[split_ratio:]\n",
        "\n",
        "train_x = [doc[0] for doc in train_documents]\n",
        "train_y = [doc[1] for doc in train_documents]\n",
        "\n",
        "test_x = [doc[0] for doc in test_documents]\n",
        "test_y = [doc[1] for doc in test_documents]\n",
        "print(f\"Розмір тренувального набору: {len(train_x)} речень.\")\n",
        "print(f\"Розмір тестового набору: {len(test_x)} речень.\")\n",
        "\n",
        "\n",
        "# 3. Побудова частотного словника\n",
        "print(\"\\nКрок 3: Побудова частотного словника...\")\n",
        "freqs = build_freqs(train_x, train_y)\n",
        "print(f\"Словник побудовано. Кількість унікальних пар ('слово', мітка): {len(freqs)}\")\n",
        "\n",
        "\n",
        "# 4. Навчання моделі\n",
        "print(\"\\nКрок 4: Навчання моделі...\")\n",
        "logprior, loglikelihood = train_naive_bayes(freqs, train_y)\n",
        "print(\"Модель навчено.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jMvOtmNg3JCm",
        "outputId": "fff1bcc1-639f-45e1-e46f-81728c3a5784"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Крок 1: Завантаження та підготовка даних з 'senseval'...\n",
            "Завантажено 6701 речень.\n",
            "Приклад речення (для слова 'hard' або 'interest'):\n",
            "Текст: now their view is one of misery -- hard earth blanketed with discarded water bottles and human waste , weary children in tattered clothes lining up for rations .\n",
            "Мітка класу: 0 (0=hard, 1=interest)\n",
            "\n",
            "Крок 2: Розділення даних...\n",
            "Розмір тренувального набору: 5360 речень.\n",
            "Розмір тестового набору: 1341 речень.\n",
            "\n",
            "Крок 3: Побудова частотного словника...\n",
            "Словник побудовано. Кількість унікальних пар ('слово', мітка): 11614\n",
            "\n",
            "Крок 4: Навчання моделі...\n",
            "Модель навчено.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 5. Оцінка точності\n",
        "print(\"\\nКрок 5: Оцінка точності на тестовому наборі...\")\n",
        "accuracy = test_naive_bayes(test_x, test_y, logprior, loglikelihood)\n",
        "print(f\"\\nТочність класифікатора: {accuracy:.4f} ({accuracy:.2%})\")\n",
        "\n",
        "\n",
        "# 6. Тестування на власному прикладі\n",
        "print(\"\\nКрок 6: Тестування на власному реченні...\")\n",
        "test_sentence_1 = \"This is a very hard task\".split()\n",
        "test_sentence_2 = \"I have no interest in this topic\".split()\n",
        "\n",
        "pred1 = naive_bayes_predict(test_sentence_1, logprior, loglikelihood)\n",
        "pred2 = naive_bayes_predict(test_sentence_2, logprior, loglikelihood)\n",
        "\n",
        "def decode_prediction(score):\n",
        "    return \"'interest'\" if score > 0 else \"'hard'\"\n",
        "\n",
        "print(f\"Речення: {' '.join(test_sentence_1)}\")\n",
        "print(f\"Прогноз: речення належить до класу {decode_prediction(pred1)}\")\n",
        "print(\"-\" * 20)\n",
        "print(f\"Речення: {' '.join(test_sentence_2)}\")\n",
        "print(f\"Прогноз: речення належить до класу {decode_prediction(pred2)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v-YiZIEw3Kt6",
        "outputId": "165e95ed-5939-477f-d22d-84b672fc3af7"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Крок 5: Оцінка точності на тестовому наборі...\n",
            "\n",
            "Точність класифікатора: 0.9806 (98.06%)\n",
            "\n",
            "Крок 6: Тестування на власному реченні...\n",
            "Речення: This is a very hard task\n",
            "Прогноз: речення належить до класу 'hard'\n",
            "--------------------\n",
            "Речення: I have no interest in this topic\n",
            "Прогноз: речення належить до класу 'interest'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import json\n",
        "\n",
        "# Перевіряємо, чи існують наші навчені параметри\n",
        "if 'logprior' in locals() and 'loglikelihood' in locals():\n",
        "\n",
        "    # Створюємо словник для експорту\n",
        "    model_to_export = {\n",
        "        'logprior': logprior,\n",
        "        'loglikelihood': loglikelihood\n",
        "    }\n",
        "\n",
        "    # Зберігаємо словник у файл model.json\n",
        "    with open('model.json', 'w', encoding='utf-8') as f:\n",
        "        json.dump(model_to_export, f, ensure_ascii=False, indent=4)\n",
        "\n",
        "    print(\"Модель успішно експортована у файл 'model.json'\")\n",
        "    # Тепер ви можете завантажити цей файл зі свого середовища (напр., з Google Colab)\n",
        "else:\n",
        "    print(\"Помилка: змінні 'logprior' та 'loglikelihood' не знайдено. Будь ласка, навчіть модель перед експортом.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hBkbo9Jn3MQt",
        "outputId": "10b2f5c6-a57e-409d-f579-11da2b572246"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Модель успішно експортована у файл 'model.json'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "OvMsK3KP4bMd"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}